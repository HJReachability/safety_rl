# Copyright (c) 2019â€“2020, The Regents of the University of California.
# All rights reserved.
#
# This file is subject to the terms and conditions defined in the LICENSE file
# included in this repository.
#
# Please contact the author(s) of this library if you have any questions.
# Authors: Neil Lugovoy   ( nflugovoy@berkeley.edu )

import ray
from ray import tune
from ray.tune import Experiment
from policy_gradient.pg import PGTrainer as PGTrainerSBE
from ray.rllib.agents.pg.pg import PGTrainer
from datetime import datetime
from ray.tune.registry import register_env
import gym
from utils import get_save_dir

# == Experiment 4 ==
"""
This experiment runs policy gradient optimizing for the Safety Bellman Equation
outcome that is induced by the backup on the cheetah environment and performs
hyper-parameter search. This is compared against policy gradient optimizing for
sum of discounted rewards with the same reward function and sum of discounted
rewards with only penalization for falling over.
"""

# Register environments (need custom gym environments for safety problem).
def cheetah_penalize_env_creator(env_config):
    from gym_reachability import gym_reachability
    return gym.make('cheetah_balance-v0')


def cheetah_env_creator(env_config):
    from gym_reachability import gym_reachability
    return gym.make('cheetah_balance_penalize-v0')

register_env('cheetah_balance-v0', cheetah_penalize_env_creator)
register_env('cheetah_balance_penalize-v0', cheetah_penalize_env_creator)

ray.init()
now = datetime.now()
date = now.strftime('%b') + str(now.day)

# =======  Hyper-Parameter Search =======
search_config = {}

# === Environment ===
search_config['env'] = 'cheetah_balance-v0'
search_config['gamma'] = 0.99
search_config['horizon'] = 45
search_config['seed'] = 0

# == Optimization ==
search_config['lr'] = tune.grid_search([1e-4, 5e-4, 1e-3])
search_config['train_batch_size'] = tune.grid_search([50, 100, 150])
search_config['sample_batch_size'] = 50

# == Scheduling ==
search_config['timesteps_per_iteration'] = int(1e5)

max_iterations = int(1e7 / search_config['timesteps_per_iteration'])
checkpoint_freq = int(1e5)

# This Experiment will call the PGTrainer constructor once at the beginning of
# the experiment and call PGTrainer._train() until the condition specified by
# the argument stop is met. In this case once the training_iteration reaches
# exp_config["max_iterations"] the experiment will stop. Every checkpoint_freq
# it will call save the policy state and at the end of the experiment. Each
# trial is a different set of hyper-parameter in this case since the config
# specifies to grid search over hyper-parameters. The data for each trial will
# be saved in local_dir/name/trial_name where local_dir and name are the
# arguments to Experiment() and trial_name is produced by ray based on the
# hyper-parameters of the trial and time of the Experiment.

# Searches over hyper-parameters for sum of discounted rewards policy gradient
# on the cheetah.
sum_cheetah_search = Experiment(
    name='pg_sum_cheetah_search_' + date,
    config=search_config,
    run=PGTrainer,
    num_samples=1,
    stop={'training_iteration': max_iterations},
    local_dir=get_save_dir(),
    checkpoint_freq=checkpoint_freq,
    checkpoint_at_end=True)

# Searches over hyper-parameters for SBE outcome policy gradient on the cheetah.
sbe_cheetah_search = Experiment(
    name='pg_sbe_cheetah_search' + date,
    config=search_config,
    run=PGTrainerSBE,  # note the difference from above
    num_samples=1,
    stop={'training_iteration': max_iterations},
    local_dir=get_save_dir(),
    checkpoint_freq=checkpoint_freq,
    checkpoint_at_end=True)

# Copying dictionary before making changes. Otherwise the previous
# experiment would be changed.
search_penalize_config = search_config.copy()
search_penalize_config['env'] = 'cheetah_balance_penalize-v0'

# Searches over hyper-parameters sum of discounted rewards policy gradient on
# the cheetah with the reward function only penalizing safety violations (see
# cheetah_balance_penalize.py).
sum_cheetah_penalize_search = Experiment(
    name='pg_sum_cheetah_penalize_search' + date,
    config=search_penalize_config,
    run=PGTrainer,
    num_samples=1,
    stop={'training_iteration': max_iterations},
    local_dir=get_save_dir(),
    checkpoint_freq=checkpoint_freq,
    checkpoint_at_end=True)

tune.run_experiments([sum_cheetah_search,
                      sum_cheetah_penalize_search,
                      sbe_cheetah_search],
                      verbose=2)

# =======  100 Seed Experiment =======
multi_seed_config = search_config.copy()

# === Environment ===
multi_seed_config['env'] = 'cheetah_balance-v0'
multi_seed_config['seed'] = tune.grid_search(list(range(100)))

# == Optimization ==
multi_seed_config['lr'] = 1e-4
multi_seed_config['train_batch_size'] = 200

# Runs sum of discounted rewards policy gradient on the cheetah on 100 seeds
# with the best hyper-parameters from the hyper-parameter search.
sum_cheetah = Experiment(
    name='pg_sum_cheetah_' + date,
    config=multi_seed_config,
    run=PGTrainer,
    num_samples=1,
    stop={'training_iteration': max_iterations},
    local_dir=get_save_dir(),
    checkpoint_freq=checkpoint_freq,
    checkpoint_at_end=True)

# == Optimization ==
sbe_multi_seed_config = multi_seed_config.copy()

# TODO need to set these based on results of hyper param search
multi_seed_config['lr'] = 1e-4
multi_seed_config['train_batch_size'] = 200

# runs SBE outcome policy gradient on the cheetah on 100 seeds with the best
# hyper-parameters from the hyper-parameter search.
sbe_cheetah = Experiment(
    name='pg_sbe_cheetah' + date,
    config=multi_seed_config,
    run=PGTrainerSBE,
    num_samples=1,
    stop={'training_iteration': max_iterations},
    local_dir=get_save_dir(),
    checkpoint_freq=checkpoint_freq,
    checkpoint_at_end=True)

# == Environment ==
penalize_multi_seed_config = multi_seed_config.copy()
penalize_multi_seed_config['env'] = 'cheetah_balance_penalize-v0'

# == Optimization ==
# TODO need to set these based on results of hyper param search
penalize_multi_seed_config['lr'] = 1e-4
penalize_multi_seed_config['train_batch_size'] = 200

# Runs sum of discounted rewards policy gradient on the penalize-only cheetah on
# 100 seeds with the best hyper-parameters from the hyper-parameter search.
sum_cheetah_penalize = Experiment(
    name='pg_sum_cheetah_penalize' + date,
    config=penalize_multi_seed_config,
    run=PGTrainer,
    num_samples=1,
    stop={'training_iteration': max_iterations},
    local_dir=get_save_dir(),
    checkpoint_freq=checkpoint_freq,
    checkpoint_at_end=True)

tune.run_experiments([sum_cheetah, sum_cheetah_penalize, sbe_cheetah],
    verbose=2)
